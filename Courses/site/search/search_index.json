{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NCC Supercomputing Luxembourg","text":"<p> Luxinnovation, the University of Luxembourg and LuxProvide are jointly managing the National Competence Centre Supercomputing in Luxembourg. Its mission is to promote the use of HPC linked to computing, data analytics and artificial intelligence and to support stakeholders such as industry \u2013 including SMEs and startups \u2013 academia and public administration to navigate the national and european HPC ecosystem. </p> <p>Luxembourg has a unique HPC infrastructure that is open to companies as well as to researchers.</p> <p>The Luxembourg National Competence Centre in HPC offers a wide portfolio of services to help you to set up and implement your HPC-enabled projects.</p> <p>Visit our webisite: National Competence Centre Supercomputing Luxembourg</p>"},{"location":"#meluxina-open-for-commercial-use","title":"MeluXina: open for commercial use","text":"<p>Inaugurated recently in June 2021, MeluXina is open for commercial use. It has received top marks by ranking organisation TOP500, which places it 1st in the EU and 4th in the world for its energy efficiency.</p> <p>Unlike most HPC systems that are pure research infrastructures, 65% of MeluXina\u2019s capacity is available for start-ups, SMEs and large companies. The system has been built on an efficient platform and is meant to serve a large variety of complex, data-driven computational workloads. Based on the Modular Supercomputing Architecture, its forward-looking design responds to the convergence of simulation, modelling, data analytics and artificial intelligence, and enables simulation driven by predictive analytics.</p>"},{"location":"#hpc-thursdays-webinar-series","title":"HPC Thursdays webinar series","text":"<p>The \u201cHPC Thursdays\u201d webinar series provides an understanding of the benefits of using a supercomputer. It also looks at what a supercomputer is, how it works, and the skills needed to use one. Each webinar presents concrete examples of applications in different domains to help participants understand the value added of using a supercomputer, and to inspire future users in order to boost their innovation activities.</p> <p>The HPC Thursdays webinars are designed for newcomers in the supercomputing world, from all sectors of industry and all disciplines of research as well as from public administration. This is the perfect exploratory path to understand the basics of HPC.</p>"},{"location":"ai/introduction/","title":"Introduction","text":"<p>The Luxembourg Competence Centre in High-Performance Computing (HPC), in collaboration with NVIDIA  and OpenACC.org, is hosting online the AI for Science and Engineering Bootcamp during 2 half-days. The first part will be dedicated to theory, and the second part will focus on hands-on challenges on GPU accelerators of the MeluXina supercomputer. For whom? </p> <p>Both current or prospective users of large hybrid CPU/GPU clusters, which develop HPC and AI applications and could benefit from GPU acceleration, are encouraged to participate! What will you learn and how? </p> <p>During this online Bootcamp, participants will learn how to apply AI tools, techniques, and algorithms to real-life problems. Participants will be introduced to the critical concepts of Deep Neural Networks, how to build Deep Learning models, and how to measure and improve the accuracy of their models. Participants will also learn essential data pre-processing techniques to ensure a robust machine-learning pipeline. The Bootcamp is a hands-on learning experience where mentors guide participants. Learning outcomes</p>"},{"location":"ai/introduction/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<pre><code>Apply Deep Convolutional Neural Networks for science and engineering applications\nUnderstand the Classification (multi-class classification) methodology in AI\nImplement AI algorithms using Keras (e.g. TensorFlow)\nUse an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set\nRun AI applications in the Jupyter notebook environment (and understand singularity containers)\n</code></pre>"},{"location":"ai/introduction/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda</p> <p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9th 2023: 01:30 PM \u2013 05:00 PM</p> <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10th  2023: 01:30 PM \u2013 05:00 PM</p> <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"cuda/","title":"Introduction to GPU programming using CUDA","text":"<p>Participants from this course will learn GPU programming using the CUDA programming model, such as synchronisation, memory allocation and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part.</p>"},{"location":"cuda/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"cuda/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture</li> <li>Threads blocks</li> </ul> </li> <li>Implement CUDA programming model<ul> <li>Programming structure</li> <li>Device calls (threads block organisation)</li> <li>Host calls</li> </ul> </li> <li>Efficient handling of memory management<ul> <li>Host to Device</li> <li>Unified memory</li> </ul> </li> <li>Apply the CUDA programming knowledge to accelerate examples from science and engineering<ul> <li>Iterative solvers from science and engineering</li> <li>Matrix multiplication, vector addition, etc</li> </ul> </li> </ul>"},{"location":"cuda/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++. No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary. </p>"},{"location":"cuda/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"cuda/exercise-1/","title":"Hello World","text":"<p>Now our first exercise would be to print out the hello world from GPU. To do that, we need to do the following things:</p> <ul> <li>Run a part or entire application on the GPU</li> <li>Call cuda function on device</li> <li>It should be called using function qualifier <code>__global__</code></li> <li>Calling the device function on the main program:</li> <li>C/C++ example, <code>c_function()</code></li> <li>CUDA example, <code>cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;()</code> (just using 1 thread)</li> <li><code>&lt;&lt;&lt; &gt;&gt;&gt;</code>, specify the threads blocks within the bracket</li> <li>Make sure to synchronize the threads</li> <li><code>__syncthreads()</code> synchronizes all the threads within a thread block</li> <li><code>CudaDeviceSynchronize()</code> synchronizes a kernel call in host</li> <li>Most of the CUDA APIs are synchronized calls by default (but sometimes    it is good to call explicit synchronized calls to avoid errors    in the computation)</li> </ul>"},{"location":"cuda/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionCUDA-version <pre><code>//-*-C++-*-\n// Hello-world.c\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\nvoid c_function()\n{\nprintf(\"Hello World!\\n\");\n}\nint main()\n{\nc_function();\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Hello-world.cu\n#include&lt;studio.h&gt;\n#include&lt;cuda.h&gt;\n// device function will be executed on device (GPU) \n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n// synchronize all the threads\n__syncthreads();\n}\nint main()\n{\n// call the kernel function \ncuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;();\n// synchronize the device kernel call\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Hello-world.c -o Hello-World-CPU\n// execution \n$ ./Hello-World-CPU\n// output\n$ Hello World from CPU!\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n// execution\n$ ./Hello-World-GPU\n// output\n$ Hello World from GPU!\n</code></pre> Question <p>Right now, you are printing just one <code>Hello World from GPU</code>, but what if you would like to print more <code>Hello World from GPU</code>? How can you do that?</p> QuestionAnswerSolution Output <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n__syncthreads();\n}\nint main()\n{\n// define your thread block here\ncuda_function&lt;&lt;&lt;&gt;&gt;&gt;();\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n__syncthreads();\n}\nint main()\n{\n// define your thread block here\ncuda_function&lt;&lt;&lt;10,1&gt;&gt;&gt;();\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> <pre><code>Hello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\n</code></pre>"},{"location":"cuda/exercise-2/","title":"Vector Addition","text":"<p>In this example, we will continue with vector addition in GPU using the CUDA programming model. This is an excellent example to begin with because we usually need to do some arithmetic operations using matrices or vectors. For that, we need to know how to access the indexes of the matrix or vector to do the computation efficiently. In this example, we will practice SIMT computation by adding two vectors.</p> <ul> <li>Memory allocation on both CPU and GPU. Because as discussed before,    GPU is an accelerator and can not act as a host machine    .So therefore, the computation    has to be initiated via CPU. That means, we need to first initialise the data on the host,    that is CPU. At the same time, we also need to initialise the memory allocation on the GPU.    Because, we need to transfer the data from a CPU to GPU.</li> </ul> <p></p> <ul> <li> <p>Allocating the CPU memory for a, b, and out vector <pre><code>// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout   = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for d_a, d_b, and d_out matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the     arrays a and b.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n</code></pre></p> </li> <li> <p>Transfer initialized value from CPU to GPU <pre><code>// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>Creating a 2D thread block <pre><code>// Thread organization \ndim3 dimGrid(1, 1, 1);    dim3 dimBlock(16, 16, 1); </code></pre></p> </li> </ul> Conversion of thread blocks <pre><code>//1D grid of 1D blocks\n__device__ int getGlobalIdx_1D_1D()\n{\nreturn blockIdx.x * blockDim.x + threadIdx.x;\n}\n//1D grid of 2D blocks\n__device__ int getGlobalIdx_1D_2D()\n{\nreturn blockIdx.x * blockDim.x * blockDim.y\n+ threadIdx.y * blockDim.x + threadIdx.x;\n}\n//1D grid of 3D blocks\n__device__ int getGlobalIdx_1D_3D()\n{\nreturn blockIdx.x * blockDim.x * blockDim.y * blockDim.z + threadIdx.z * blockDim.y * blockDim.x\n+ threadIdx.y * blockDim.x + threadIdx.x;\n}\n//2D grid of 1D blocks \n__device__ int getGlobalIdx_2D_1D()\n{\nint blockId   = blockIdx.y * gridDim.x + blockIdx.x;\nint threadId = blockId * blockDim.x + threadIdx.x; return threadId;\n}\n//2D grid of 2D blocks  \n__device__ int getGlobalIdx_2D_2D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * (blockDim.x * blockDim.y) +\n(threadIdx.y * blockDim.x) + threadIdx.x;\nreturn threadId;\n}\n//2D grid of 3D blocks\n__device__ int getGlobalIdx_2D_3D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n+ (threadIdx.z * (blockDim.x * blockDim.y))\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n}\n//3D grid of 1D blocks\n__device__ int getGlobalIdx_3D_1D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * blockDim.x + threadIdx.x;\nreturn threadId;\n}\n//3D grid of 2D blocks\n__device__ int getGlobalIdx_3D_2D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * (blockDim.x * blockDim.y)\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n}\n//3D grid of 3D blocks\n__device__ int getGlobalIdx_3D_3D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n+ (threadIdx.z * (blockDim.x * blockDim.y))\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n</code></pre> <ul> <li> <p>Calling the kernel function <pre><code>// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p> </li> <li> vector addition function call Serial-versionCUDA-version <pre><code>// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) {\nfor(int i = 0; i &lt; n; i ++)\n{\nout[i] = a[i] + b[i];\n}\nreturn out;\n}\n</code></pre> <pre><code>// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronize all the threads \n__syncthreads();\n}\n</code></pre> </li> </ul> <p> </p> <ul> <li> <p>Copy back computed value from GPU to CPU <pre><code>// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Vector-addition.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) {\nfor(int i = 0; i &lt; n; i ++)\n{\nout[i] = a[i] + b[i];\n}\nreturn out;\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;       // Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout = (float*)malloc(sizeof(float) * N);\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Start measuring time\nclock_t start = clock();\n// Executing CPU function \nVector_Add(a, b, out, N);\n// Stop measuring time and calculate the elapsed time\nclock_t end = clock();\ndouble elapsed = (double)(end - start)/CLOCKS_PER_SEC;\nprintf(\"Time measured: %.3f seconds.\\n\", elapsed);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition-template.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {     // allign your thread id indexes \nint i = ........\n// Allow the   threads only within the size of N\nif------\n{\nout[i] = a[i] + b[i];\n}\n// Synchronize all the threads \n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)......\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a,......\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = ....\nb[i] = ....\n}\n// Transfer data from host to device memory\ncudaMemcpy.....\n// Thread organization \ndim3 dimGrid....  dim3 dimBlock....\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt; &gt;&gt;&gt;....\n// Transfer data back to host memory\ncudaMemcpy....\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree...\n// Deallocate host memory\nfree..\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout = (float*)malloc(sizeof(float) * N);\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n// Thread organization \ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1); // execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n// execution \n$ ./Vector-Addition-CPU\n// output\n$ ./Vector-addition-CPU out[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Vector-addition.cu -o Vector-Addition-GPU\n// execution\n$ ./Vector-Addition-GPU\n// output\n$ ./Vector-addition-GPU\nout[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>What happens if you remove the <code>__syncthreads();</code> from the <code>__global__ void vector_add(float *a, float *b,     float *out, int n)</code> function.</li> <li>Can you remove the if condition <code>if(i &lt; n)</code> from the <code>__global__ void vector_add(float *a, float *b,    float *out, int n)</code> function. If so how can you do that?</li> <li>Here we do not use the <code>cudaDeviceSynchronize()</code> in the main application, can you figure out why we     do not need to use it. </li> <li>Can you create a different kinds of threads block for larger number of array?</li> </ul>"},{"location":"cuda/exercise-3/","title":"Matrix Multiplication","text":"<p>We will now look into the basic matrix multiplication. In this example, we will perform the matrix multiplication. Matrix multiplication involves a nested loop. Again, most of the time, we might end up doing computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. </p> <p></p> b <ul> <li> <p>Allocating the CPU memory for A, B, and C matrix.    Here we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *a, *b, *c;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for A, B, and C matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_c;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n</code></pre></p> </li> <li> <p>Transfer initialized A and B matrix from CPU to GPU <pre><code>cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>2D thread block for indexing x and y <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></p> </li> <li> <p>Calling the kernel function <pre><code>// Device function call\nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n</code></pre></p> </li> <li> matrix multiplication function call serialcuda <pre><code>float * matrix_mul(float *h_a, float *h_b, float *h_c, int width)\n{\nfor(int row = 0; row &lt; width ; ++row)\n{\nfor(int col = 0; col &lt; width ; ++col)\n{\nfloat temp = 0;\nfor(int i = 0; i &lt; width ; ++i)\n{\ntemp += h_a[row*width+i] * h_b[i*width+col];\n}\nh_c[row*width+col] = temp;\n}\n}\nreturn h_c;\n}\n</code></pre> <pre><code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\nint row = blockIdx.x * blockDim.x + threadIdx.x;\nint col = blockIdx.y * blockDim.y + threadIdx.y;\nif ((row &lt; width) &amp;&amp; (col &lt; width)) {\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n</code></pre> </li> <li> <p>Copy back computed value from GPU to CPU;    transfer the data back to GPU (from device to host).    Here is the C matrix that contains the product of the two matrices. <pre><code>// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Matrix-multiplication.c\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\nfloat * matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float temp = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     temp += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = temp;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix )size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c;       // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Device function call \nmatrix_mul(a, b, c, N);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\ncout &lt;&lt; c[j] &lt;&lt;\" \";\n}\ncout &lt;&lt; \" \" &lt;&lt;endl;\n}\n// Deallocate host memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication-template.cu\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\n__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\n// create a 2d threads block\nint row = ..................\nint col = ....................\n// only allow the threads that are needed for the computation \nif (................................)\n{\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\n...\n...\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n...\n...\n// Transfer data from host to device memory\ncudaMemcpy(.........................);\ncudaMemcpy(.........................);\n// Thread organization\nint blockSize = ..............;\ndim3 dimBlock(......................);\ndim3 dimGrid(.......................);\n// Device function call \nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// CPU computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree...\n// Deallocate host memory\nfree...\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication.cu\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\n__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\nint row = blockIdx.x * blockDim.x + threadIdx.x;\nint col = blockIdx.y * blockDim.y + threadIdx.y;\nif ((row &lt; width) &amp;&amp; (col &lt; width)) {\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n// Device function call \nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-CPU\n// execution \n$ ./Matrix-Multiplication-CPU\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n16 16 16 16 16 16 16 16  16 16 16 16  16 16 16 16 </code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\n// execution\n$ ./Matrix-Multiplication-GPU\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number\n$ 256\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try with 2D array matrix multiplication with 2D thread block?</li> <li>Can you get the correct soltion if you remove the <code>if ((row &lt; width) &amp;&amp; (col &lt; width))</code> condition from the <code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)</code> function?</li> <li>Please try with different thread blocks and different matrix sizes. <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/","title":"Shared Memory","text":"<p>In this example, we try shared memory matrix multiplication. This is achieved by blocking the global matrix into a small block matrix (tiled matrix) that can fit into the shared memory of the Nvidia GPU. Shared memory from the GPUs, which has a good bandwidth within the GPUs compared to access to the global memory.</p> <p> </p> <ul> <li>This is very similar to the previous example; however, we just need to allocate the small block matrix into shared memory.  The below example shows the blocking size for a and b matrix respectively for gobal A and B matrix.  <pre><code>  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n</code></pre></li> </ul> <p></p> <ul> <li>Then we need to iterate elements within the block size and, finally with the global index.  These can be achieved with CUDA threads. </li> </ul> <p></p> <ul> <li>You can also increase the shared memory or L1 cache size by using <code>cudaFuncSetCacheConfig</code>. For more information about  CUDA API, please refer to cudaFuncSetCacheConfig.</li> </ul> Tips <pre><code>cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n//cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n\ncudaFuncCachePreferNone: no preference for shared memory or L1 (default)\ncudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache\ncudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory\ncudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory\n\n// simple example usage increasing more shared memory \n#include&lt;stdio.h&gt;\nint main()\n{\n  // example of increasing the shared memory \n  cudaDeviceSetCacheConfig(My_Kernel, cudaFuncCachePreferShared);\n  My_Kernel&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize(); \n  return 0;\n}\n</code></pre> <ul> <li>Different Nvidia GPUs provides different configuration, for example, Ampere GA102 GPU Architecture, will support the following configuration: <pre><code>128 KB L1 + 0 KB Shared Memory\n120 KB L1 + 8 KB Shared Memory\n112 KB L1 + 16 KB Shared Memory\n96 KB L1 + 32 KB Shared Memory\n64 KB L1 + 64 KB Shared Memory\n28 KB L1 + 100 KB Shared Memory\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-templateMatrix-multiplication-shared.cu <pre><code>// Matrix-multiplication-shared-template.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n// block size for the matrix \n#define BLOCK_SIZE 16\nusing namespace std;\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, float *d_c, int width)\n{\n// Shared memory allocation for the block matrix  \n__shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n...\n// Indexing for the block matrix\nint tx = threadIdx.x;\n...\n// Indexing global matrix to block matrix \nint row = threadIdx.x+blockDim.x*blockIdx.x;\n...\n// Allow threads only for size of rows and columns (we assume square matrix)\nif ((row &lt; width) &amp;&amp; (col&lt; width))\n{\n// Save temporary value for the particular index\nfloat temp = 0;\nfor(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n{\n// Allign the global matrix to block matrix \na_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\nb_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n// Make sure all the threads are synchronized\n....\n// Multiply the block matrix \nfor(int j = 0; j &lt; BLOCK_SIZE; ++j)\n{\ntemp += a_block[ty][j] * b_block[j][tx];    }\n// Make sure all the threads are synchronized\n...\n}\n// Save block matrix entry to global matrix \n...\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float temp = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     temp += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = temp;                            }                                                         }   return h_c;           }\nint main()\n{  cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N=0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\ndim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                ...\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// Cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\ncout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> <pre><code>// Matrix-multiplication-shared.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n// block size for the matrix \n#define BLOCK_SIZE 16\nusing namespace std;\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, float *d_c, int width)\n{\n// Shared memory allocation for the block matrix  \n__shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n__shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n// Indexing for the block matrix\nint tx = threadIdx.x;\nint ty = threadIdx.y;\n// Indexing global matrix to block matrix \nint row = threadIdx.x+blockDim.x*blockIdx.x;\nint col = threadIdx.y+blockDim.y*blockIdx.y;\n// Allow threads only for size of rows and columns (we assume square matrix)\nif ((row &lt; width) &amp;&amp; (col&lt; width))\n{\n// Save temporary value for the particular index\nfloat temp = 0;\nfor(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n{\n// Allign the global matrix to block matrix \na_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\nb_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n// Make sure all the threads are synchronized\n__syncthreads(); // Multiply the block matrix\nfor(int j = 0; j &lt; BLOCK_SIZE; ++j)\n{\ntemp += a_block[ty][j] * b_block[j][tx];    }\n__syncthreads();\n}\n// Save block matrix entry to global matrix \nd_c[row*width+col] = temp;\n}\n}\n// Host call (matix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{  cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N=0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\ndim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                dim3 Grid_dim(ceil(N/BLOCK_SIZE), ceil(N/BLOCK_SIZE), 1);\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\ncout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> Compilation and Output CUDA-version <pre><code>// compilation\n$ nvcc -arch=sm_70 Matrix-multiplication-shared.cu -o Matrix-multiplication-shared\n// execution\n$ ./Matrix-multiplication-shared\nProgramme assumes that matrix size is N*N Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\nPlease enter the N size number\n$ 256\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Could you resize the <code>BLOCK_SIZE</code> number and check the solution's correctness?</li> <li>Can you also create a different kind of thread block and matrix size and check the solution's correctness?</li> <li>Please try with <code>cudaFuncSetCacheConfig</code> and check if you can successfully execute the application. </li> </ul>"},{"location":"cuda/exercise-5/","title":"Unified Memory","text":"<p>Unified memory simplifies the explicit data movement from host to device by programmers. CUDA API will manage the data transfer between CPU and GPU. In this example, we will look into vector addition in GPU using the unified memory concept.</p> <ul> <li>Just one memory allocation is enough <code>cudaMallocManaged()</code>.  The blow table summerise the required steps needed for the unified memory concept. </li> </ul> <p></p> Without unified memory With unified memory Allocate the host memory Allocate the host memory Allocate the device memory Allocate the device memory Initialize the host value Initialize the host value Transfer the host value to the device memory location Transfer the host value to the device memory location Do the computation using the CUDA kernel Do the computation using the CUDA kernel Transfer the data from the device to host Transfer the data from the device to host Free device memory Free device memory Free host memory Free host memory"},{"location":"cuda/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Unified Memory - Vector Addition Without Unified MemoryWith Unified Memory - templateWith Unified Memory-version <pre><code>//-*-C++-*-\n// Without-unified-memory.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out; // Allocate host memory\na = (float*)malloc(sizeof(float) * N);\nb = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n// Thread organization \ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1);\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n/*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device(unified) memory\ncudaMallocManaged......\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\nd_a[i] = ...\nd_b[i] = ...\n}\n/*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n// Thread organization \ndim3 dimGrid...    dim3 dimBlock...\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// synchronize if needed\n......\n/*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", d_out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device(unified) memory\ncudaFree...\n/*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// With-unified-memory.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n/*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMallocManaged(&amp;d_a, sizeof(float) * N);\ncudaMallocManaged(&amp;d_b, sizeof(float) * N);\ncudaMallocManaged(&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\nd_a[i] = 1.0f;\nd_b[i] = 2.0f;\n}\n/*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n// Thread organization\ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1);\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\ncudaDeviceSynchronize();\n/*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", d_out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n/*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\nreturn 0;\n}\n</code></pre> Compilation and Output Without-unified-memory.cuWith-unified-memory <pre><code>// compilation\n$ nvcc -arch=compute_70 Without-unified-memory.cu -o Without-Unified-Memory\n// execution \n$ ./Without-Unified-Memory\n// output\n$ ./Without-Unified-Memory\nout[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 With-unified-memory.cu -o With-Unified-Memory\n// execution\n$ ./With-Unified-Memory\n// output\n$ ./With-Unified-Memory out[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>Here in this example, we have used <code>cudaDeviceSynchronize()</code>; can you remove <code>cudaDeviceSynchronize()</code>   and still get a correct solution? if not, why (think)?</li> <li>Please try with different thread blocks and array sizes. </li> </ul>"},{"location":"cuda/preparation/","title":"Preparation","text":""},{"location":"cuda/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"cuda/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/CUDA .\n[u100490@login03 u100490]$ cd CUDA/\n[u100490@login03 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@login03 CUDA]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200117   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li>7.1 Reserve the interactive node for running/testing CUDA applications    <pre><code>$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></li> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#8-now-we-need-to-check-simple-cuda-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple CUDA application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 CUDA]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code  <pre><code>$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n### or\n$ source module.sh\n</code></pre></p> </li> <li> check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>For example, Dry-run-test  <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<pre><code>$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre> <ul> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:</li> </ul> <pre><code>[u100490@mel2063 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@mel2063 CUDA]$ ls\n[u100490@mel2063 CUDA]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Shared-memory  Vector-addition\n[u100490@mel2063 CUDA]$ source module.sh\n[u100490@mel2063 CUDA]$ cd Hello-world\n// compilation\n[u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n[u100490@mel2063 CUDA]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 CUDA]$ Hello World from GPU\n</code></pre>"},{"location":"cuda/profiling/","title":"Profiling and Performance","text":""},{"location":"cuda/profiling/#time-measurement","title":"Time measurement","text":"<p>In CUDA, the execution time can be measured by using the cuda events. CUDA API events shall be created using <code>cudaEvent_t</code>, for example, <code>cudaEvent_t start, stop;</code>. And thereafter, it can be initiated by <code>cudaEventCreate(&amp;start)</code> for start and similarly for stop, it can be created as <code>cudaEventCreate(&amp;stop)</code>. </p> CUDA API <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start,0);\n</code></pre> <p>And it can be initialised to measure the timing as <code>cudaEventRecord(start,0)</code> and <code>cudaEventRecord(stop,0)</code>. Then the timings can be measured as float, for example, <code>cudaEventElapsedTime(&amp;time, start, stop)</code>. Finally, all the events should be destroyed using <code>cudaEventDestroy</code>, for example, <code>cudaEventDestroy(start)</code> and <code>cudaEventDestroy(start)</code>.</p> CUDA API <pre><code>cudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n</code></pre> <p>The following example shows how to measure your GPU kernel call in a CUDA application:</p> Example <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start);\n\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n//use CUDA API to stop the measuring time\ncudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n\ncout &lt;&lt; \" time taken for the GPU kernel\" &lt;&lt; time &lt;&lt; endl;\n</code></pre>"},{"location":"cuda/profiling/#nvidia-system-wide-performance-analysis","title":"Nvidia system-wide performance analysis","text":"<p>Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are:</p> <ul> <li> <p>Nsight Compute: CUDA application interactive kernel profiler: This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. <code>ncu -o profile Application.exe</code> command will create an output file <code>profile.ncu-rep</code> which can be opened using <code>ncu-ui</code>. </p> </li> <li> Example <pre><code>$ ncu ./a.out\nmatrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7\nSection: GPU Speed Of Light Throughput\n---------------------------------------------------------------------- --------------- ------------------------------\nDRAM Frequency                                                           cycle/usecond                         874.24\nSM Frequency                                                             cycle/nsecond                           1.31\nElapsed Cycles                                                                   cycle                         241109\nMemory [%]                                                                           %                          13.68\nDRAM Throughput                                                                      %                           0.07\nDuration                                                                       usecond                         184.35\nL1/TEX Cache Throughput                                                              %                          82.39\nL2 Cache Throughput                                                                  %                          13.68\nSM Active Cycles                                                                 cycle                       30531.99\nCompute (SM) [%]                                                                     %                           1.84\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n      waves across all SMs. Look at Launch Statistics for more details.                                             \n\nSection: Launch Statistics\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Size                                                                                                       1024\nFunction Cache Configuration                                                                  cudaFuncCachePreferNone\nGrid Size                                                                                                          16\nRegisters Per Thread                                                   register/thread                             26\nShared Memory Configuration Size                                                  byte                              0\nDriver Shared Memory Per Block                                              byte/block                              0\nDynamic Shared Memory Per Block                                             byte/block                              0\nStatic Shared Memory Per Block                                              byte/block                              0\nThreads                                                                         thread                          16384\nWaves Per SM                                                                                                     0.10\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80             \n      multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n      concurrently with other workloads, consider reducing the block size to have at least one block per            \n      multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n      Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n      description for more details on launch configurations.                                                        \n\nSection: Occupancy\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Limit SM                                                                   block                             32\nBlock Limit Registers                                                            block                              2\nBlock Limit Shared Mem                                                           block                             32\nBlock Limit Warps                                                                block                              2\nTheoretical Active Warps per SM                                                   warp                             64\nTheoretical Occupancy                                                                %                            100\nAchieved Occupancy                                                                   %                          45.48\nAchieved Active Warps Per SM                                                      warp                          29.11\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n      theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads   \n      or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n      as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n      (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n      optimizing occupancy.                                                                                         \n</code></pre> </li> <li> <p>Nsight Graphics: Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. </p> </li> <li> <p>Nsight Systems: System-wide performance analysis tool: It is needed when we try to do heterogeneous computation profiling, for example, mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use <code>$ nsys profile --help</code></p> </li> <li> Example <pre><code>$ nsys profile -t nvtx,cuda --stats=true ./a.out\nGenerating '/scratch_local/nsys-report-ddd1.qdstrm'\n[1/7] [========================100%] report1.nsys-rep\n[2/7] [========================100%] report1.sqlite\n[3/7] Executing 'nvtxsum' stats report\nSKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/7] Executing 'cudaapisum' stats report\n\nTime (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)  Min (ns)  Max (ns)   StdDev (ns)        Name      \n--------  ---------------  ---------  -----------  --------  --------  ---------  -----------  ----------------\n    99.7        398381310          3  132793770.0    8556.0      6986  398365768  229992096.8  cudaMalloc      \n     0.2           714256          3     238085.3   29993.0     24944     659319     364807.8  cudaFree        \n     0.1           312388          3     104129.3   43405.0     37692     231291     110162.3  cudaMemcpy      \n     0.0            51898          1      51898.0   51898.0     51898      51898          0.0  cudaLaunchKernel\n\n[5/7] Executing 'gpukernsum' stats report\n\nTime (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)     GridXYZ         BlockXYZ                        Name                   \n--------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------  --------------  ------------------------------------------\n100.0           181949          1  181949.0  181949.0    181949    181949          0.0     4    4    1    32   32    1  matrix_mul(float *, float *, float *, int)\n\n[6/7] Executing 'gpumemtimesum' stats report\n\nTime (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n--------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n 75.0            11520      2    5760.0    5760.0      5760      5760          0.0  [CUDA memcpy HtoD]\n 25.0             3840      1    3840.0    3840.0      3840      3840          0.0  [CUDA memcpy DtoH]\n\n[7/7] Executing 'gpumemsizesum' stats report\n\nTotal (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n----------  -----  --------  --------  --------  --------  -----------  ------------------\n  0.080      2     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy HtoD]\n  0.040      1     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy DtoH]\n\nGenerated:\n   /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep\n   /m100/home/userexternal/ekrishna/Teaching/report1.sqlite\n</code></pre> </li> </ul>"},{"location":"cuda/profiling/#occupancy","title":"Occupancy","text":"<p>The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.</p> <p>\\(Occupancy  = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\)</p> <ul> <li> Examples Occupancy CUDACompilation and results <pre><code>//-*-C++-*-\n#include&lt;iostream&gt;\n// Device code\n__global__ void MyKernel(int *d, int *a, int *b)\n{\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nd[idx] = a[idx] * b[idx];\n}\n// Host code\nint main()\n{\n// set your numBlocks and blockSize to get 100% occupancy\nint numBlocks = 32;        // Occupancy in terms of active blocks\nint blockSize = 128;\n// These variables are used to convert occupancy to warps\nint device;\ncudaDeviceProp prop;\nint activeWarps;\nint maxWarps;\ncudaGetDevice(&amp;device);\ncudaGetDeviceProperties(&amp;prop, device);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(\n&amp;numBlocks,\nMyKernel,\nblockSize,0);\nactiveWarps = numBlocks * blockSize / prop.warpSize;\nmaxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\nstd::cout &lt;&lt; \"Max # of Blocks : \" &lt;&lt; numBlocks &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"ActiveWarps : \" &lt;&lt; activeWarps &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"MaxWarps : \" &lt;&lt; maxWarps &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"Occupancy: \" &lt;&lt; (double)activeWarps / maxWarps * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 occupancy.cu -o Occupancy-GPU\n// execution\n$ ./Occupancy-GPU\n// output\nMax number of Blocks : 16\nActiveWarps : 64\nMaxWarps : 64\nOccupancy: 100%\n</code></pre> </li> </ul> Questions <ul> <li>Occupancy: can you change <code>numBlocks</code> and <code>blockSize</code> in Occupancy.cu code  and check how it affects or predicts the occupancy of the given Nvidia microarchitecture?</li> <li>Profiling: run your <code>Matrix-multiplication.cu</code> and <code>Vector-addition.cu</code> code and observe what you notice?  for example, how to improve the occupancy? Or maximise a GPU utilization?</li> <li>Timing: using CUDA events API can you measure your GPU kernel execution, and compare how fast is your GPU computation compared to CPU computation?</li> </ul>"},{"location":"openacc/","title":"Introduction to OpenACC for Heterogeneous Computing","text":"<p>Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openacc/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openacc/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture </li> <li>Threads blocks </li> </ul> </li> <li>Implement OpenACC programming model  <ul> <li>Compute constructs  </li> <li>Loop constructs </li> <li>Data clauses</li> </ul> </li> <li>Efficient handling of memory management  <ul> <li>Host to Device </li> <li>Unified memory </li> </ul> </li> <li>Apply the OpenACC programming knowledge to accelerate examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openacc/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous. </p>"},{"location":"openacc/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openmp/","title":"Introduction to OpenMP Programming for Shared Memory Parallel Architecture","text":"<p>Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openmp/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openmp/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the shared memory architecture <ul> <li>Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA)  </li> <li>Hybrid distributed shared memory architecture  </li> </ul> </li> <li>Implement OpenMP programming model  <ul> <li>Parallel region  </li> <li>Environment routines  </li> <li>Data sharing  </li> </ul> </li> <li>Efficient handling of OpenMP constructs  <ul> <li>Work sharing  </li> <li>Synchronisation constructs  </li> <li>Single Instruction Multiple Data (SIMD) directive </li> </ul> </li> <li>Apply the OpenMP programming knowledge to parallelise examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openmp/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed.</p>"},{"location":"openmp/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openmp/exercise-1/","title":"Parallel Region","text":""},{"location":"openmp/exercise-1/#parallel-construct","title":"Parallel Construct","text":"<p>In this exercise, we will create a parallel region and execute the computational content in parallel. First, however, this exercise is to create a parallel region and understand the threads' behaviour in parallel. In later exercises, we will study how to parallelise the computational task within the parallel region.</p> <p></p> <p>To create a parallel region, we use the following parallel constructs:</p> <p>Parallel Constructs</p> C/C++FORTRAN <pre><code>#pragma omp parallel\n</code></pre> <pre><code>!$omp parallel \n</code></pre> <p>The above figure illustrates the parallel region behaviour; as we notice, within the parallel region, we get parallel threads. This means parallel threads can be executed independently of each other, and there is no order of execution.</p> <p>At the same time, in order to enable OpenMP constructs, clauses, and environment variables. etc., we need to include the OpenMP library as follows:</p> <p>OpenMP library</p> C/C++FORTRAN <pre><code>#include&lt;omp.h&gt;\n</code></pre> <pre><code>use omp_lib\n</code></pre>"},{"location":"openmp/exercise-1/#compilers","title":"Compilers","text":"<p>The following compilers would support the OpenMP programming model.</p> <ul> <li>GNU - It is an opensource and can be used for Intel and AMD CPUs</li> <li>Intel - It is from Intel and only optimized for Intel CPUs</li> <li>AOOC - Suitable for AMD CPUs, especially \u201cZen\u201d core architecture.</li> </ul> <p>Examples (GNU, Intel and AMD): Compilation</p> GNUIntelAOOC <pre><code>$ gcc test.c -fopenmp\n$ g++ test.cc -fopenmp\n$ gfortran test.f90 -fopenmp\n</code></pre> <pre><code>$ icc test.c -qopenmp\n$ icpc test.cc -qopenmp\n$ ifort test.f90 -qopenmp        </code></pre> <pre><code>$ clang test.c -fopenmp\n$ clang++ test.cc -fopenmp\n$ flang test.f90 -fopenmp        </code></pre>"},{"location":"openmp/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN) <pre><code>#include&lt;iostream&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; endl;\ncout &lt;&lt; \"Hello world from master thread\"&lt;&lt; endl;\ncout &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_Serial\nprint *, 'Hello world from master thread'\nend program\n</code></pre> <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\ncout &lt;&lt; endl;\n// creating the parallel region (with N number of threads)\n#pragma omp parallel\n{\ncout &lt;&lt; \"Hello world from parallel region \"&lt;&lt; endl;\n} // parallel region is closed\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\nprint *, 'Hello world from master thread'\n!$omp parallel\nprint *, 'Hello world from parallel region'\n!$omp end parallel\nprint *,'end of the programme from master thread'\nend program\n</code></pre> Compilation and Output Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN) <pre><code>// compilation\n$ g++ Hello-world-Serial.cc -o Hello-World-Serial\n// execution \n$ ./Hello-World-Serial\n// output\n$ Hello world from master thread\n</code></pre> <pre><code>// compilation\n$ gfortran Hello-world-Serial.f90 -o Hello-World-Serial\n// execution \n$ ./Hello-World-Serial\n// output\n$ Hello world from master thread\n</code></pre> <pre><code>// compilation\n$ g++ -fopenmp Hello-world-OpenMP.cc -o Hello-World-OpenMP\n// execution\n$ ./Hello-World-OpenMP\n// output\n$ Hello world from parallel region\nHello world from parallel region\n..\n..\nHello world from parallel region\nend of the programme from master thread\n</code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Hello-world-OpenMP.f90 -o Hello-World-OpenMP\n// execution\n$ ./Hello-World-OpenMP\n// output\n$ Hello world from master thread\nHello world from parallel region\n..\n..\nHello world from parallel region\nend of the programme from master thread\n</code></pre> Questions <ul> <li>What do you notice from those examples? Can you control parallel region printout, that is, how many times it should be printed or executed?     </li> <li>What happens if you do not use the OpenMP library, <code>#include&lt;omp.h&gt; or use omp_lib</code>?</li> </ul> <p>Although creating a parallel region would allow us to do the parallel computation, however, at the same time, we should have control over the threads being created in the parallel region, for example, how many threads are needed for a particular computation, thread number, etc. For this, we need to know a few of the important environment routines which are provided by OpenMP. The below list shows a few of the most important environment routines that should be known by the programmer for optimised OpenMP coding.</p>"},{"location":"openmp/exercise-1/#environment-routines-important","title":"Environment Routines (important)","text":"<ul> <li> <p>Define number of threads to be used within the parallel region</p> <pre><code>(C/C++): void omp_set_num_threads(int num_threads);\n(FORTRAN): subroutine omp_set_num_threads(num_threads) \ninteger num_threads\n</code></pre> </li> <li> <p>To get number of threads in the current parallel region</p> <pre><code>(C/C++): int omp_get_num_threads(void);\n(FORTRAN): integer function omp_get_num_threads()\n</code></pre> </li> <li> <p>To get available maximum threads (system default)</p> <pre><code>(c/c++): int omp_get_max_threads(void);\n(FORTRAN): integer function omp_get_max_threads()\n</code></pre> </li> <li> <p>To get thread numbers (e.g., 1, 4, etc.)</p> <pre><code>(c/c+): int omp_get_thread_num(void);\n(FORTRAN): integer function omp_get_thread_num()\n</code></pre> </li> <li> <p>To know number processors available to the device</p> <pre><code>(c/c++): int omp_get_num_procs(void);\n(FROTRAN): integer function omp_get_num_procs()\n</code></pre> </li> </ul>"},{"location":"openmp/exercise-1/#questions-and-solutions_1","title":"Questions and Solutions","text":"Questions <ul> <li>How can you identify the thread numbers within the parallel region?</li> <li>What happens if you not set <code>omp_set_num_threads()</code>, for example, <code>omp_set_num_threads(5)|call omp_set_num_threads(5)</code>, what do you notice? </li> <li>Alternatively, you can also set a number of threads to be used in the application while the compilation <code>export OMP_NUM_THREADS</code>; what do you see?</li> </ul> Question (C/C++)Question (FORTRAN)Answer (C/C++)Answer (FORTRAN)AnswerSolution Output (C/C++)Solution Output (FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\ncout &lt;&lt; endl;\n// creating the parallel region (with N number of threads)\n#pragma omp parallel\n{\n//cout &lt;&lt; \"Hello world from thread id \"\n&lt;&lt; \" from the team size of \"\n&lt;&lt; endl;\n} // parallel region is closed\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n!$omp parallel \n!! print *, \n!$omp end parallel\nend program\n</code></pre> <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\ncout &lt;&lt; endl;\n// creating the parallel region (with N number of threads)\n#pragma omp parallel\n{\ncout &lt;&lt; \"Hello world from thread id \"\n&lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n&lt;&lt; omp_get_num_threads()\n&lt;&lt; endl;\n} // parallel region is closed\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\nend program\n</code></pre> <pre><code>$ export OMP_NUM_THREADS=10\n// or \n$ setenv OMP_NUM_THREADS 4\n// or\n$ OMP NUM THREADS=4 ./omp code.exe\n</code></pre> <pre><code>ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of  from the team size of 10\n0 from the team size of 10\n10\n10\n10\n7 from the team size of 10\n4 from the team size of 10\n8 from the team size of 10\n1 from the team size of 10\n6 from the team size of 10\n</code></pre> <pre><code>Hello world from thread id            0 from the team size of          10\nHello world from thread id            4 from the team size of          10\nHello world from thread id            5 from the team size of          10\nHello world from thread id            9 from the team size of          10\nHello world from thread id            2 from the team size of          10\nHello world from thread id            3 from the team size of          10\nHello world from thread id            7 from the team size of          10\nHello world from thread id            6 from the team size of          10\nHello world from thread id            8 from the team size of          10\nHello world from thread id            1 from the team size of          10\n</code></pre>"},{"location":"openmp/exercise-1/#utilities","title":"Utilities","text":"<p>The main aim is to do the parallel computation to speed up computation on a given parallel architecture. Therefore, measuring the timing and comparing the solution between serial and parallel code is very important. In order to measure the timing, OpenMP provides an environmental variable, <code>omp_get_wtime();</code>.</p> Time measuring C/C++FORTRAN <pre><code>double start; \ndouble end; \nstart = omp_get_wtime(); \n... work to be timed ... \nend = omp_get_wtime(); \nprintf(\"Work took %f seconds\\n\", end - start);\n</code></pre> <pre><code>DOUBLE PRECISION START, END \nSTART = omp_get_wtime() \n... work to be timed ... \nEND = omp_get_wtime() \nPRINT *, \"Work took\", END - START, \"seconds\"        \n</code></pre>"},{"location":"openmp/exercise-2/","title":"Data Sharing Attribute","text":""},{"location":"openmp/exercise-2/#shared-variable","title":"Shared variable","text":"<ul> <li>All the threads have access to the shared variable.</li> <li>By default in the parallel region, all the variables are  considered as a shared variable expect the loop iteration  counter variables.</li> </ul> <p>Note</p> <p>Shared variables should be handled carefully; otherwise it causes race conditions in the program.</p> <p></p> Examples: Shared variable (C/C++)(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n// Array size\nint N = 10;\n// Initialize the variables\nfloat *a;\n// Allocate the memory\na  = (float*)malloc(sizeof(float) * N);\n//#pragma omp parallel for\n// or \n#pragma omp parallel for shared(a)\nfor (int i = 0; i &lt; N; i++)\n{\na[i] = a[i] + i;  cout &lt;&lt; \"value of a in the parallel region\" &lt;&lt; a[i] &lt;&lt; endl;\n}\nfor (int i = 0; i &lt; N; i++)\ncout &lt;&lt; \"value of a after the parallel region \" &lt;&lt; a[i] &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\ninteger :: n, i\nn=10\n! Allocate memory for vector\nallocate(a(n))\n!$omp parallel shared(a)\n!$omp do\ndo i = 1, n\na(i) = a(i) + i\nprint*, 'value of a in the parallel region', a(i)\nend do\n!$omp end do\n!$omp end parallel\ndo i = 1, n\na(i) = a(i) + i\nprint*,'value of a after the parallel region', a(i)\nend do\n! Delete the memory\ndeallocate(a)\nend program main\n</code></pre> Question <ul> <li>Does the value of vector <code>a</code> change after the parallel loop, if not why, think?</li> <li>Do we really need to mention <code>shared(a)</code>, is it neccessary? </li> </ul>"},{"location":"openmp/exercise-2/#private-variable","title":"Private variable","text":"<ul> <li>Each thread will have its own copy of the private variable.</li> <li>And the private variable is only accessible within the parallel region,  not outside of the parallel region.</li> <li>By default, the loop iteration counters are considered as a private.</li> <li>A change made by one thread is not visible to other threads.</li> </ul> Examples: Private variable (C/C++)(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n// Array size\nint N = 10;\n// Initialize the variables\nfloat *a,b,c;\nb = 1.0;\nc = 2.0;\n// Allocate the memory\na  = (float*)malloc(sizeof(float) * N);\n#pragma omp parallel for private(b,c)\nfor (int i = 0; i &lt; N; i++)\n{\nb = a[i] + i;\nc = b + 10 * i;\ncout &lt;&lt; \"value of c in the parallel region \" &lt;&lt; c &lt;&lt; endl;\n}\ncout &lt;&lt; \"value of c after the parallel region \" &lt;&lt; c &lt;&lt; endl; return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8) :: b, c\ninteger :: n, i  n=10\nb=1.0\nc=2.0\n! Allocate memory for vector\nallocate(a(n))\n!$omp parallel private(b,c) shared(a)\n!$omp do\ndo i = 1, n\nb = a(i) + i\nc = b + 10 * i\nprint*, 'value of c in the parallel region', c\nend do\n!$omp end do\n!$omp end parallel\nprint*, 'value of c after the parallel region', c\n! Delete the memory\ndeallocate(a)\nend program main\n</code></pre> Questions <ul> <li>What is the value of the varible <code>a</code> in the parallel region and after the parallel region?</li> <li>After the parallel region, does variable <code>a</code> has been updated or not? </li> </ul>"},{"location":"openmp/exercise-2/#lastprivate","title":"Lastprivate","text":"<ul> <li>lastprivate: is also similar to a private clause</li> <li>But each thread will have an uninitialized copy of the variables passed  as lastprivate</li> <li>At the end of the parallel loop or sections, the final variable value will  be the last thread accessed value in the section or in a parallel loop.</li> </ul> Examples: Lastprivate variable (C/C++)(FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\nint n = 10;\nint var = 5;\nomp_set_num_threads(10);\n#pragma omp parallel for lastprivate(var)\nfor(int i = 0; i &lt; n; i++)\n{\nvar += omp_get_thread_num();\ncout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n} /*-- End of parallel region --*/\ncout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\nreturn 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n! Initialise the variable\nreal(8) :: var\ninteger :: n, i  n = 10\nvar = 5\ncall omp_set_num_threads(10)\n!$omp parallel !$omp do lastprivate(var)\ndo i = 1, n\nvar  =  var + omp_get_thread_num()\nprint*, 'lastprivate in the parallel region ', var\nend do\n!$omp end do\n!$omp end parallel\nprint*, 'lastprivate after the parallel region ', var\nend program main\n</code></pre> Questions <ul> <li>What is the value of the varible <code>var</code> in the parallel region and after the parallel region?</li> <li>Do you think the initial value of varibale <code>var</code> is been considered within the parallel region? </li> </ul>"},{"location":"openmp/exercise-2/#firstprivate","title":"Firstprivate","text":"<ul> <li>firstprivate: is similar to a private clause</li> <li>But each thread will have an initialized copy of the variables passed  as firstprivate</li> <li>Available for parallel constructs, loop, sections and single  constructs</li> </ul> Examples: Firstprivate variable (C/C++)(FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\nint n = 10;\nint var = 5;\nomp_set_num_threads(10);\n#pragma omp parallel for firstprivate(var)\nfor(int i = 0; i &lt; n; i++)\n{\nvar += omp_get_thread_num();\ncout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n} /*-- End of parallel region --*/\ncout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\nreturn 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n! Initialise the variable\nreal(8) :: var\ninteger :: n, i  n = 10\nvar = 5\ncall omp_set_num_threads(10)\n!$omp parallel !$omp do firstprivate(var)\ndo i = 1, n\nvar  =  var + omp_get_thread_num()\nprint*, 'lastprivate in the parallel region ', var\nend do\n!$omp end do\n!$omp end parallel\nprint*, 'lastprivate after the parallel region ', var\nend program main\n</code></pre> Questions <ul> <li>What is the value of the varible <code>var</code> in the parallel region and after the parallel region?</li> <li>Is variable <code>var</code> has been updated after the parallel region, if not why, think?</li> </ul>"},{"location":"openmp/exercise-3/","title":"Work Sharing Constructs(loop)","text":""},{"location":"openmp/exercise-3/#serial-version-discussion","title":"Serial version discussion","text":"<p>To begin to understand the work-sharing constructs, we need to learn how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loop. For this, we will learn simple vector addition examples.</p> <p></p> <p>As we can see from the above figure, the two vectors should be added to get a single vector. This is done by iterating over the elements and adding them together. For this, we use <code>for - C/C++</code> or <code>do - FORTRAN</code>.  Since there are no data dependencies, the loop indexes do not have any data dependency on the other indexes. Therefore, it is easy to parallelise.</p> Examples: Loop Serial(C/C++)Serial(FORTRAN) <pre><code>for(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\n</code></pre> <pre><code>do i = 1, n\nc(i) = a(i) + b(i)\nend do\n</code></pre> <p>Note</p> <p>FORTRAN has a column-major order and C/C++ has a row-major order</p> <pre><code>Fortran array index starts from 1\nC/C++ arrray index starts from 0\n</code></pre>"},{"location":"openmp/exercise-3/#parallel-version-discussion","title":"Parallel version discussion","text":"<p>Now we will look into the how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loops. For this, we just need to add below syntax (OpenMP directives).</p> Functionality Syntax in C/C++ Syntax in FORTRAN Distribute iterations over the threads #pragma omp for !$omp do <p>With the help of the above syntax the loops can be easily parallelised. The figure below shows an example of how the loops are parallelised. As we can notice here, we set the <code>omp_set_num_threads(5)</code> for the number of parallel threads that should be used within the loops. Furthermore, the loop index goes from <code>0</code> to <code>9</code>; in total, we need to iterate <code>10</code> elements. </p> <p>In this example, using <code>5</code> threads would divide <code>10</code> iterations by <code>two</code>. Therefore, each thread will handle <code>2</code> iterations. In total, <code>5</code> threads will do just <code>2</code> iterations in parallel for <code>10</code> elements.  </p> <p></p> Examples: Loops parallelisation Serial(C/C++)FORTRAN(C/C++) <pre><code>#pragma omp parallel for\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\n//or\n#pragma omp parallel \n#pragma omp for\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\n</code></pre> <pre><code>!$omp parallel do\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n//or\n!$omp parallel\n!$omp do\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre> <p>Form understating loop parallelisation, we will continue with vector operations in parallel, that is, adding two vectors. It is very simple, and we just need to add the <code>#pragma omp parallel for</code> for C/C++, <code>!$omp parallel do</code> for FORTRAN. Could you try this by yourself? The serial code, templates and compilation command have been provided as follows.</p>"},{"location":"openmp/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>//-*-C++-*-\n// Vector-addition.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) {\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\nreturn c;\n}\nint main()\n{\n// Initialize the variables\nfloat *a, *b, *c;       // Allocate the memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n// Initialize the arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Start measuring time\nclock_t start = clock();\n// Executing vector addtion function \nVector_Add(a, b, c, N);\n// Stop measuring time and calculate the elapsed time\nclock_t end = clock();\ndouble elapsed = (double)(end - start)/CLOCKS_PER_SEC;\nprintf(\"Time measured: %.3f seconds.\\n\", elapsed);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"c[0] = %f\\n\", c[0]);\nprintf(\"PASSED\\n\");\n// Deallocate the memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  implicit none contains\nsubroutine Vector_Addition(a, b, c, n)\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\nend subroutine Vector_Addition\nend module Vector_Addition_Mod\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b ! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i  print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n! Call the vector add subroutine call Vector_Addition(a, b, c, n)\n!!Verification\ndo i = 1, n\nif (abs(c(i)-(a(i)+b(i)) == 0.00000)) then else\nprint *, \"FAIL\"\nendif\nenddo\nprint *, \"PASS\"\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) {\n// ADD YOUR PARALLEL REGION FOR THE LOOP\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\nreturn c;\n}\nint main()\n{\n// Initialize the variables\nfloat *a, *b, *c;       // Allocate the memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n// Initialize the arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Start measuring time\nclock_t start = clock();\n// ADD YOUR PARALLEL REGION HERE  \n// Executing vector addtion function \nVector_Add(a, b, c, N);\n// Stop measuring time and calculate the elapsed time\nclock_t end = clock();\ndouble elapsed = (double)(end - start)/CLOCKS_PER_SEC;\nprintf(\"Time measured: %.3f seconds.\\n\", elapsed);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"c[0] = %f\\n\", c[0]);\nprintf(\"PASSED\\n\");\n// Deallocate the memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  implicit none contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!! ADD YOUR PARALLEL DO LOOP\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\nend subroutine Vector_Addition\nend module Vector_Addition_Mod\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b ! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i  print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n!! ADD YOUR PARALLEL REGION ! Call the vector add subroutine call Vector_Addition(a, b, c, n)\n!!Verification\ndo i = 1, n\nif (abs(c(i)-(a(i)+b(i)) == 0.00000)) then else\nprint *, \"FAIL\"\nendif\nenddo\nprint *, \"PASS\"\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) #pragma omp for\n// ADD YOUR PARALLE\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\nreturn c;\n}\nint main()\n{\n// Initialize the variables\nfloat *a, *b, *c;       // Allocate the memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n// Initialize the arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Start measuring time\nclock_t start = clock();\n#pragma omp parallel \n// Executing vector addtion function \nVector_Add(a, b, c, N);\n// Stop measuring time and calculate the elapsed time\nclock_t end = clock();\ndouble elapsed = (double)(end - start)/CLOCKS_PER_SEC;\nprintf(\"Time measured: %.3f seconds.\\n\", elapsed);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"c[0] = %f\\n\", c[0]);\nprintf(\"PASSED\\n\");\n// Deallocate the memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  implicit none contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!$omp do\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\n!$omp end do\nend subroutine Vector_Addition\nend module Vector_Addition_Mod\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b ! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i  print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n!$omp parallel ! Call the vector add subroutine call Vector_Addition(a, b, c, n)\n!$omp end parallel\n!!Verification\ndo i = 1, n\nif (abs(c(i)-(a(i)+b(i)) == 0.00000)) then else\nprint *, \"FAIL\"\nendif\nenddo\nprint *, \"PASS\"\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> Compilation and Output Serial(C/C++)Serial(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>// compilation\n$ gcc Vector-addition-Serial.c -o Vector-addition-Serial\n// execution \n$ ./Vector-addition-Serial\n// output\n$ ./Vector-addition-Serial\n</code></pre> <pre><code>// compilation\n$ gfortran Vector-addition-Serial.f90 -o Vector-addition-Serial\n// execution\n$ ./Vector-addition-Serial\n// output\n$ ./Vector-addition-Serial\n</code></pre> <pre><code>// compilation\n$ gcc -fopennmp Vector-addition-OpenMP-solution.c -o Vector-addition-Solution\n// execution \n$ ./Vector-addition-Solution\n// output\n$ ./Vector-addition-Solution\n</code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Vector-addition-OpenMP-solution.f90 -o Vector-addition-Solution\n// execution\n$ ./Vector-addition-Solution\n// output\n$ ./Vector-addition-Solution\n</code></pre> Questions <ul> <li>Can you measure the performance speedup for parallelising loop? Do you see any speedup?</li> <li>For example, can you create more threads to speed up the computation? If yer or not, why?</li> </ul>"},{"location":"openmp/exercise-4/","title":"Work Sharing Constructs(loop-scheduling)","text":""},{"location":"openmp/exercise-4/#loop-scheduling","title":"Loop scheduling","text":"<p>However, the above example is very simple.    Because, in most cases, we would end up doing a large list of arrays with complex computations within the loop.    Therefore, the work loading should be optimally distributed among the threads in those cases.    To handle those considerations, OpenMP has provided the following loop-sharing clauses. They are: <code>Static</code>, <code>Dynamic</code>, <code>Guided</code>, <code>Auto</code>, and, <code>Runtime</code>.</p> <p></p> Example - Loop scheduling clauses Serial(C/C++)FORTRAN(C/C++) <pre><code>#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\n//or\n#pragma omp parallel \n#pragma omp for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n{\nc[i] = a[i] + b[i];\n}\n</code></pre> <pre><code>!$omp parallel do schedule(static)\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n//or\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\nc(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre>"},{"location":"openmp/exercise-4/#static","title":"Static","text":"<ul> <li>The number of iterations are divided by chunksize. </li> <li>If the chunksize is not provided, a number of iterations will be divided by the size of the team of threads.<ul> <li>e.g., n=100, numthreads=5; each thread will execute the 20 iterations in parallel.</li> </ul> </li> <li>This is useful when the computational cost is similar to each iteration.</li> </ul> Examples and Question: static OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nint main()\n{\nint N = 10;\nomp_set_num_threads(5);\n#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; N; i++)\n{\ncout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    }  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i  n = 10\ncall omp_set_num_threads(5)\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>Thread id           0\nThread id           0\nThread id           4\nThread id           4\nThread id           3\nThread id           3\nThread id           2\nThread id           2\nThread id           1\nThread id           1\n</code></pre> <ul> <li>What happens if you would set the chunksize, for example, <code>schedule(static,4)</code>? What do you notice?</li> </ul>"},{"location":"openmp/exercise-4/#dynamic","title":"Dynamic","text":"<ul> <li>The number of iterations are divided by chunksize.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when the computational cost is different in the iteration.</li> <li>This will quickly place the chunk of data in the queue.</li> </ul> Examples and Question: dynamic OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nint main()\n{\nint N = 10;\nomp_set_num_threads(5);\n#pragma omp parallel for schedule(dynamic)\nfor(int i = 0; i &lt; N; i++)\n{\ncout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    }  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i  n = 10\ncall omp_set_num_threads(5)\n!$omp parallel\n!$omp do schedule(dynamic)\ndo i = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>Thread id  Thread id 20 Thread id 4 Thread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id\nThread id 1 3\n</code></pre> <ul> <li>What happens if you would set the chunksize, for example, schedule(dynamic,4)? What do you notice?</li> <li>Do you notice if the iterations are divided by the chunksize that we set?</li> </ul>"},{"location":"openmp/exercise-4/#guided","title":"Guided","text":"<ul> <li>Similar to dynamic scheduling, the number of iterations are divided by chunksize.</li> <li>But the chunk of the data size is decreasing, which is proportional to the number of unsigned iterations divided by the number of threads.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when there is poor load balancing at the end of the iteration.</li> </ul> Examples and Question: guided OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nint main()\n{\nint N = 10;\nomp_set_num_threads(5);\n#pragma omp parallel for schedule(guided)\nfor(int i = 0; i &lt; N; i++)\n{\ncout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    }  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i  n = 10\ncall omp_set_num_threads(5)\n!$omp parallel\n!$omp do schedule(guided)\ndo i = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>Thread id Thread id   Thread id0 41\nThread id 0\nThread id 4\nThread id 4\nThread id 2\nThread id 2\nThread id 3 Thread id\n</code></pre> <ul> <li>Are there any differences between <code>auto</code> and <code>guided</code> or <code>dynamic</code>?</li> </ul>"},{"location":"openmp/exercise-4/#auto","title":"Auto","text":"<ul> <li>Here the compiler chooses the best combination of the chunksize to be used. </li> </ul> Examples and Question: auto OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nint main()\n{\nint N = 10;\nomp_set_num_threads(5);\n#pragma omp parallel for schedule(auto)\nfor(int i = 0; i &lt; N; i++)\n{\ncout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    }  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i  n = 10\ncall omp_set_num_threads(5)\n!$omp parallel\n!$omp do schedule(auto)\ndo i = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>Thread id Thread id Thread id    Thread id0 34 Thread id Thread id 0\n1\nThread id 1\nThread id 3\n2\nThread id 2\nThread id 4\n</code></pre> <ul> <li>What would you choose for your application, auto, dynamic, guided, or static? If you are going to choose either one of them, then have a valid reason. </li> </ul>"},{"location":"openmp/exercise-4/#runtime","title":"Runtime","text":"<ul> <li>During the compilation, we simply set the loop scheduling concept.</li> </ul> Example:Loop scheduling clauses - runtime Compilation <pre><code>setenv OMP_SCHEDULE=\"guided,4\" setenv OMP_SCHEDULE=\"dynamic\" setenv OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n// or\nexport OMP_SCHEDULE=\"guided,4\" export OMP_SCHEDULE=\"dynamic\" export OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n</code></pre> Examples and Question: runtime OpenMP(C/C++)OpenMP(FORTRAN)Compilation <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nint main()\n{\nint N = 10;\nomp_set_num_threads(5);\n#pragma omp parallel for schedule(runtime)\nfor(int i = 0; i &lt; N; i++)\n{\ncout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    }  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i  n = 10\ncall omp_set_num_threads(5)\n!$omp parallel\n!$omp do schedule(runtime)\ndo i = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>export OMP_SCHEDULE=\"dynamic,3\"\n// check if you have exported the environment value\n$ env | grep OMP_SCHEDULE\n$ OMP_SCHEDULE=dynamic,3 // if you want to unset\n$ unset OMP_SCHEDULE\n$ env | grep OMP_SCHEDULE\n// it(OMP_SCHEDULE=dynamic,3) will be removed\n</code></pre>"},{"location":"openmp/exercise-5/","title":"Other Worksharing Constructs","text":"<p>Most of the time, we end up having more than one loop, a nested loop, where two or three loops will be next to each other. OpenMP provides a clause for handling this kind of situation with <code>collapse</code>. To understand this, we will now study Matrix multiplication, which involves a nested loop. Again, most of the time, we might do computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future.</p> <p></p>"},{"location":"openmp/exercise-5/#collapse","title":"Collapse","text":"<p>The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided number of threads. The figure below shows an example of not using a <code>collapse</code> clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. </p> <p></p> <p>This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we might have more threads available on a machine; for example, on MeluXina, we can have up to 256 threads. Therefore, when adding the <code>collapse</code> clause, we notice that the available threads execute every single iteration, as seen in the figure below.</p> <p></p> Collapse C/C++FORTRAN <pre><code>#pragma omp parallel\n#pragma omp for collapse(2)\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{     cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n}\n}\n// Or\n#pragma omp parallel for collapse(2)\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{ cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n}\n}\n</code></pre> <pre><code>!$omp parallel\n!$omp do collapse(2) do i = 1, n\ndo j = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\nend do\n!$omp end do\n!$omp end parallel\n!! Or\n!$omp parallel do collapse(2)\ndo i = 1, n\ndo j = 1, n\nprint*, 'Thread id', omp_get_thread_num()\nend do\nend do\n!$omp end parallel do\n</code></pre> Examples and Questions: Collapse OpenMP(C/C++)OpenMP(FORTRAN)Output(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\nint N=5;\n#pragma omp parallel\n#pragma omp for collapse(2)\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\ncout &lt;&lt; \"Outer loop id \" &lt;&lt; i &lt;&lt; \" Inner loop id \"&lt;&lt; j &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n}\n}\nreturn 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\ninteger :: n, i, j  n=5\n!$omp parallel\n!$omp do collapse(2) do i = 1, n\ndo j = 1, n\nprint*, 'Outer loop id ', i , 'Inner loop id ', j , 'Thread id', omp_get_thread_num()\nend do\nend do\n!$omp end do\n!$omp end parallel\nend program main\n</code></pre> <pre><code>Outer loop id            4 Inner loop id            2 Thread id          16\nOuter loop id            1 Inner loop id            4 Thread id           3\nOuter loop id            5 Inner loop id            1 Thread id          20\nOuter loop id            4 Inner loop id            1 Thread id          15\nOuter loop id            2 Inner loop id            1 Thread id           5\nOuter loop id            3 Inner loop id            1 Thread id          10\nOuter loop id            3 Inner loop id            4 Thread id          13\nOuter loop id            4 Inner loop id            4 Thread id          18\nOuter loop id            4 Inner loop id            3 Thread id          17\nOuter loop id            3 Inner loop id            3 Thread id          12\nOuter loop id            1 Inner loop id            2 Thread id           1\nOuter loop id            2 Inner loop id            3 Thread id           7\nOuter loop id            1 Inner loop id            5 Thread id           4\nOuter loop id            2 Inner loop id            2 Thread id           6\nOuter loop id            3 Inner loop id            2 Thread id          11\nOuter loop id            2 Inner loop id            5 Thread id           9\nOuter loop id            3 Inner loop id            5 Thread id          14\nOuter loop id            5 Inner loop id            3 Thread id          22\nOuter loop id            5 Inner loop id            4 Thread id          23\nOuter loop id            5 Inner loop id            5 Thread id          24\nOuter loop id            2 Inner loop id            4 Thread id           8\nOuter loop id            1 Inner loop id            3 Thread id           2\nOuter loop id            4 Inner loop id            5 Thread id          19\nOuter loop id            1 Inner loop id            1 Thread id           0\nOuter loop id            5 Inner loop id            2 Thread id          21\n</code></pre> <ul> <li>Can you add here any of the scheduling clauses, for example, static, dynamic, etc?</li> <li>Is it really necessary to them when you use <code>collapse</code>, or is it dependent on other factors, such as the nature of the    computation and available threads?</li> </ul>"},{"location":"openmp/exercise-5/#reduction","title":"Reduction","text":"<p>The reduction clauses are data-sharing attribute clauses that can be used to perform some forms of repetition calculations in the parallel region.</p> <ul> <li>it can be used for arithmetic reductions: +,*,-,max,min</li> <li>and also with logical operator reductions in C: &amp; &amp;&amp; | || \u02c6</li> </ul> Reduction C/C++FORTRAN <pre><code>#pragma omp parallel\n#pragma omp for reduction(+:sum)\nfor(int i = 0; i &lt; N; i++)\n{\nsum +=a[i];\n}\n// Or\n#pragma omp parallel for reduction(+:sum)\nfor(int i = 0; i &lt; N; i++)\n{\nsum += a[i];\n}\n</code></pre> <pre><code>!$omp parallel\n!$omp do reduction(+:sum)\ndo i = 1, n\nsum = sum + a(i)\nend do\n!$omp end do\n!$omp end parallel\n!! Or\n!$omp parallel do reduction(+:sum)\ndo i = 1, n\nsum = sum + a(i)\nend do\n!$omp end parallel do\n</code></pre> Examples and Question: Reduction OpenMP(C/C++)OpenMP(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\nint sum,N = 10;\nfloat *a = (float*)malloc(sizeof(float) * N);\n#pragma omp parallel for reduction(+:sum)\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = i;\nsum += a[i];\n}\ncout &lt;&lt; \"Sum is \"&lt;&lt; sum &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\ninteger :: n, i, sum\nn=10\n! Allocate memory for vector\nallocate(a(n))\n!$omp parallel do reduction(+:sum)\ndo i = 1, n\na(i) = i\nsum = sum + a(i)\nend do\n!$omp end parallel do\nprint *, 'Sum is ', sum\nend program main\n</code></pre> <ul> <li>What happens if you do not use the reduction clause? Do we still get the correct answer?</li> </ul>"},{"location":"openmp/exercise-5/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>In this example, we consider a square matrix; <code>M=N</code> is equal for both <code>A</code> and <code>B</code> matrices. Even though we deal here with a 2D matrix, we create a 1D array to represent a 2D matrix. In this example,  we must use <code>collapse</code> clause since matrix multiplication deals with 3 loops. The first 2 outer loops will take rows of the <code>A</code> matrix and columns of the <code>B</code> matrix. Therefore, these two loops can be easily parallelised. But then we need to sum the value of the those two outer loops value finally; this is where we should use the <code>reduction</code> clause. </p> matrix multiplication function call C/C++FORTRAN <pre><code>for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)\n{\nsum=0;\nfor(int i = 0; i &lt; width ; ++i)                         {                                                     sum += a[row*width+i] * b[i*width+col];      }                                                     c[row*width+col] = sum;                           }\n} </code></pre> <pre><code>do row = 0, width-1\ndo col = 0, width-1\nsum=0\ndo i = 0, width-1\nsum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\nenddo\nc(row*width+col+1) = sum\nenddo\nenddo\n</code></pre>"},{"location":"openmp/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   { float sum = 0;\nfor(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)\n{\nsum=0;\nfor(int i = 0; i &lt; width ; ++i)                         {                                                     sum += a[row*width+i] * b[i*width+col];      }                                                     c[row*width+col] = sum;                           }\n}   }\nint main()\n{  printf(\"Programme assumes that matrix size is N*N \\n\");\nprintf(\"Please enter the N size number \\n\");\nint N =0;\nscanf(\"%d\", &amp;N);\n// Initialize the memory\nfloat *a, *b, *c;       // Allocate memory\na = (float*)malloc(sizeof(float) * (N*N));\nb = (float*)malloc(sizeof(float) * (N*N));\nc = (float*)malloc(sizeof(float) * (N*N));\n// Initialize arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Fuction call \nMatrix_Multiplication(a, b, c, N);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nprintf(\"%f \", c[j]);\n}\nprintf(\"\\n\");\n}\n// Deallocate memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>module Matrix_Multiplication_Mod  implicit none contains\nsubroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\ndo row = 0, width-1\ndo col = 0, width-1\nsum=0\ndo i = 0, width-1\nsum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\nenddo\nc(row*width+col+1) = sum\nenddo\nenddo\nend subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n! Call the vector add subroutine call Matrix_Multiplication(a, b, c, n)\n!!Verification\ndo i=1,n*n\nprint *, c(i)\nenddo\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   { float sum = 0;\nfor(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)\n{\nsum=0;\nfor(int i = 0; i &lt; width ; ++i)                         {                                                     sum += a[row*width+i] * b[i*width+col];      }                                                     c[row*width+col] = sum;                           }\n}   }\nint main()\n{  printf(\"Programme assumes that matrix size is N*N \\n\");\nprintf(\"Please enter the N size number \\n\");\nint N =0;\nscanf(\"%d\", &amp;N);\n// Initialize the memory \nfloat *a, *b, *c;       // Allocate memory\na = (float*)malloc(sizeof(float) * (N*N));\nb = (float*)malloc(sizeof(float) * (N*N));\nc = (float*)malloc(sizeof(float) * (N*N));\n// Initialize arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Fuction call \nMatrix_Multiplication(a, b, c, N);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nprintf(\"%f \", c[j]);\n}\nprintf(\"\\n\");\n}\n// Deallocate memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code> module Matrix_Multiplication_Mod  implicit none contains\nsubroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n!!! ADD LOOP PARALLELISATION\ndo row = 0, width-1\ndo col = 0, width-1\nsum=0\ndo i = 0, width-1\nsum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\nenddo\nc(row*width+col+1) = sum\nenddo\nenddo\nend subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n!!!! ADD PARALLEL REGION ! Call the vector add subroutine call Matrix_Multiplication(a, b, c, n)\n!!Verification\ndo i=1,n*n\nprint *, c(i)\nenddo\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   { float sum = 0;\n#pragma for loop collapse(2) reduction (+:sum)\nfor(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)\n{\nsum=0;\nfor(int i = 0; i &lt; width ; ++i)                         {                                                     sum += a[row*width+i] * b[i*width+col];      }                                                     c[row*width+col] = sum;                           }\n}   }\nint main()\n{  printf(\"Programme assumes that matrix size is N*N \\n\");\nprintf(\"Please enter the N size number \\n\");\nint N =0;\nscanf(\"%d\", &amp;N);\n// Initialize the memory\nfloat *a, *b, *c;       // Allocate memory\na = (float*)malloc(sizeof(float) * (N*N));\nb = (float*)malloc(sizeof(float) * (N*N));\nc = (float*)malloc(sizeof(float) * (N*N));\n// Initialize arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n#pragma omp parallel\n// Fuction call \nMatrix_Multiplication(a, b, c, N);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nprintf(\"%f \", c[j]);\n}\nprintf(\"\\n\");\n}\n// Deallocate memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>module Matrix_Multiplication_Mod  implicit none contains\nsubroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n!$omp do collapse(2) reduction(+:sum)\ndo row = 0, width-1\ndo col = 0, width-1\nsum=0\ndo i = 0, width-1\nsum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\nenddo\nc(row*width+col+1) = sum\nenddo\nenddo\n!$omp end do\nend subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ninteger :: n, i print *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n! Initialize content of input vectors, ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\na(i) = sin(i*1D0) * sin(i*1D0)\nb(i) = cos(i*1D0) * cos(i*1D0) enddo\n!$omp parallel\n! Call the vector add subroutine call Matrix_Multiplication(a, b, c, n)\n!$omp end parallel\n!!Verification\ndo i=1,n*n\nprint *, c(i)\nenddo\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\nend program main\n</code></pre> Compilation and Output Serial-version(C/C++)Serial-version(FORTRAN)OpenMP(C/C++)OpenMP(FORTRAN) <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-Serial\n// execution \n$ ./Matrix-Multiplication-Serial\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n8 8 8 8 8 8 8 8  8 8 8 8  8 8 8 8 </code></pre> <pre><code>// compilation\n$ gfortran Matrix-multiplication.f90 -o Matrix-Multiplication-Serial\n// execution \n$ ./Matrix-Multiplication-Serial\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n8 8 8 8 8 8 8 8  8 8 8 8  8 8 8 8 </code></pre> <pre><code>// compilation\n$ gcc -fopenmp Matrix-multiplication-Solution.c -o Matrix-Multiplication-Solution\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n8 8 8 8 8 8 8 8  8 8 8 8  8 8 8 8 </code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Matrix-multiplication-Solution.f90 -o Matrix-Multiplication-Solution\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n8 8 8 8 8 8 8 8  8 8 8 8  8 8 8 8 </code></pre> Questions <ul> <li>Right now, we are dealing with square matrices. Could you write a code for a different matrix size while still fulfilling the matrix multiplication condition?</li> </ul> <ul> <li>Could you use any one of the loop scheduling, for example, <code>dynamic</code> or <code>static</code>? Do you see any performance gain?</li> </ul>"},{"location":"openmp/exercise-6/","title":"SIMD and Others","text":""},{"location":"openmp/exercise-6/#questions-and-solutions","title":"Questions and Solutions","text":"Questions <ul> <li>How can you identify the thread numbers within the parallel region?</li> <li>What happens if you not set <code>omp_set_num_threads()</code>, for example, <code>omp_set_num_threads(5)|call omp_set_num_threads(5)</code>, what do you notice? </li> <li>Alternatively, you can also set a number of threads to be used in the application while the compilation <code>export OMP_NUM_THREADS</code>; what do you see?</li> </ul> Question (C/C++)Question (FORTRAN)Answer (C/C++)Answer (FORTRAN)AnswerSolution Output (C/C++)Solution Output (FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\ncout &lt;&lt; endl;\n// creating the parallel region (with N number of threads)\n#pragma omp parallel\n{\n//cout &lt;&lt; \"Hello world from thread id \"\n&lt;&lt; \" from the team size of \"\n&lt;&lt; endl;\n} // parallel region is closed\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n!$omp parallel \n!! print *, \n!$omp end parallel\nend program\n</code></pre> <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\ncout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\ncout &lt;&lt; endl;\n// creating the parallel region (with N number of threads)\n#pragma omp parallel\n{\ncout &lt;&lt; \"Hello world from thread id \"\n&lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n&lt;&lt; omp_get_num_threads()\n&lt;&lt; endl;\n} // parallel region is closed\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\nend program\n</code></pre> <pre><code>$ export OMP_NUM_THREADS=10\n// or \n$ setenv OMP_NUM_THREADS 4\n// or\n$ OMP NUM THREADS=4 ./omp code.exe\n</code></pre> <pre><code>ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of  from the team size of 10\n0 from the team size of 10\n10\n10\n10\n7 from the team size of 10\n4 from the team size of 10\n8 from the team size of 10\n1 from the team size of 10\n6 from the team size of 10\n</code></pre> <pre><code>Hello world from thread id            0 from the team size of          10\nHello world from thread id            4 from the team size of          10\nHello world from thread id            5 from the team size of          10\nHello world from thread id            9 from the team size of          10\nHello world from thread id            2 from the team size of          10\nHello world from thread id            3 from the team size of          10\nHello world from thread id            7 from the team size of          10\nHello world from thread id            6 from the team size of          10\nHello world from thread id            8 from the team size of          10\nHello world from thread id            1 from the team size of          10\n</code></pre>"},{"location":"openmp/preparation/","title":"Preparation","text":""},{"location":"openmp/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"openmp/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/OpenMP .\n[u100490@login03 u100490]$ cd OpenMP/\n[u100490@login03 OpenMP]$ pwd\n/project/home/p200117/u100490/OpenMP\n[u100490@login03 OpenMP]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200117   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li>7.1 Reserve the interactive node for running/testing CUDA applications    <pre><code>$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></li> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#8-now-we-need-to-check-simple-cuda-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple CUDA application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 OpenMP]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code  <pre><code>$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n### or\n$ source module.sh\n</code></pre></p> </li> <li> check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"openmp/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>For example, Dry-run-test  <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<pre><code>$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre> <ul> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"openmp/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:</li> </ul> <pre><code>[u100490@mel2063 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@mel2063 CUDA]$ ls\n[u100490@mel2063 CUDA]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Shared-memory  Vector-addition\n[u100490@mel2063 CUDA]$ source module.sh\n[u100490@mel2063 CUDA]$ cd Hello-world\n// compilation\n[u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n[u100490@mel2063 CUDA]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 CUDA]$ Hello World from GPU\n</code></pre>"},{"location":"openmp/profiling/","title":"Profiling and Performance","text":"<p>Profiling is an important task to be considered when a computer code is written. Writing parallel code is less challenging, but making it more efficient on a given parallel architecture is challenging. Moreover,  from the programming and programmer\u2019s perspective, we want to know where the code spends most of its time. In particular, we would like to know if the code (given algorithm) is compute bound, memory bound, cache misses, memory leak, proper vectorisation, cache misses, register spilling, or hot spot (time-consuming part in the code). Plenty of tools are available to profile a scientific code (computer code for doing arithmetic computing using processors). However, Here, we will focus few of the widely used tools.</p> <ul> <li>AMD uProf</li> <li>ARM Forge</li> <li>Intel tools</li> </ul>"},{"location":"openmp/profiling/#arm-forge","title":"ARM Forge","text":"<p>Arm Forge is another standard commercial tool for debugging, profiling, and analysing scientific code on the massively parallel computer architecture. They have a separate toolset for each category with the common environment: DDT for debugging, MAP for profiling, and performance reports for analysis. It also supports the MPI, UPC, CUDA, and OpenMP programming models for a different architecture with different variety of compilers. DDT and MAP will launch the GUI, where we can interactively debug and profile the code. Whereas <code>perf-report</code> will provide the analysis results in <code>.html</code> and <code>.txt</code> files.</p> Example: ARM Forge C/C++FORTRAN <pre><code># compilation with debugging tool\n$ gcc test.c -g -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n# for debugging\n$ ddt ./a .out\n# for profiling\n$ map ./a .out\n# for analysis\n$ perf-report ./a .out\n</code></pre> <pre><code># compilation \n$ gfortran test.f90 -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n# for debugging\n$ ddt ./a .out\n# for profiling\n$ map ./a .out\n# for analysis\n$ perf-report ./a .out\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-tools","title":"Intel tools","text":""},{"location":"openmp/profiling/#intel-application-snapshot","title":"Intel Application Snapshot","text":"<p>Intel Application Performance Snapshot tool helps to find essential performance factors and the metrics of CPU utilisation, memory access efficiency, and vectorisation. <code>aps -help</code> will list out profiling metrics options in APS</p> <p></p> Example: APS C/C++FORTRAN <pre><code># compilation\n$ icc -qopenmp test.c\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre> <pre><code># compilation\n$ ifort -qopenmp test.f90\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-inspector","title":"Intel Inspector","text":"<p>Intel Inspector detects and locates the memory, deadlocks, and data races in the code. For example, memory access and memory leaks can be found.</p> Example: Intel Inspector C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp example.c\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2020/12/12 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/12/12 01:20:25] ===\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.f90\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2023/05/10 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/05/10 01:20:25] ===\n</code></pre>"},{"location":"openmp/profiling/#intel-advisor","title":"Intel Advisor","text":"<p>Intel Advisor: set of collection tools for the metrics and traces that can be used for further tunning in the code. <code>survey</code>: analyse and explore an idea about where to add efficient vectorisation.</p> Example: Intel Advisor C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp test.c\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.90\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-vtune","title":"Intel VTune","text":"<ul> <li>Identifying the time consuming part in the code.</li> <li>And also the identify the cache misses and latency.</li> </ul> Example: Intel VTune C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp test.c\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of VTune amplifier\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.90\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of VTune amplifier\n</code></pre> <p><code>amplxe-cl</code> will list out the analysis types and <code>amplxe-cl -hlep</code> report will list out available reports in VTune.</p>"},{"location":"openmp/profiling/#amd-uprof","title":"AMD uProf","text":"<p>AMD uProf profiler follows a statistical sampling-based approach to collect profile data to identify the performance bottlenecks in the application.</p> Example: Intel VTune C/C++FORTRAN <pre><code># compile the code\n$ clang -fopenmp test.c\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre> <pre><code># compile the code\n$ flang -fopenmp test.90\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre>"}]}